import csv
import threading
import sys
import subprocess
import shlex
import argparse
import re
from pathlib import Path
import os
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))  # rfdiffusion/目录
PARENT_DIR = os.path.dirname(ROOT_DIR)  # 上级目录（包含dssp/和rfdiffusion/）
sys.path.append(PARENT_DIR)  # 把上级目录加入搜索路径
from pdbrepair import fix_pdb_file
from dssp.dssp import run_dssp
from dssp.dsspcsv import dssp_to_csv
import pandas as pd
import shutil

def parse_args():
    parser = argparse.ArgumentParser(description='Protein backbone remodeling', formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('--rfdiffusion_prefix', type=str, help='Path of samples generated by rfdiffusion (including prefix)')
    parser.add_argument('--inpaint_str', type=str, help='Redesigned sequence segments, such as [10-20]')
    parser.add_argument('--ss', type=str, help='Secondary structure type, such as helix and strand')
    parser.add_argument('--threshold', type=float, default=0.6, help='Threshold for DSSP analysis')
    parser.add_argument("--final_report_folder", type=str, default=None,
                        help="Folder for storing final rfdiffusion_report.csv (default: same as output_folder)")
    return parser.parse_args()



#修正蛋白质骨架pdb文件，使之可以被dssp识别
def fix_pdb(path,output_folder):
    folder_path, file_prefix = path.rsplit('/', 1)
    pattern = re.compile(f"^{file_prefix}_\d+\.pdb$")
    target_folder = Path(folder_path)
    matched_files = [
        file for file in target_folder.iterdir()  # 遍历当前文件夹所有条目
        if file.is_file()  # 仅保留文件（过滤文件夹）
           and pattern.match(file.name)  # 正则匹配文件名
    ]
    if not os.path.exists(output_folder):  ##新建文件夹
        os.makedirs(output_folder, exist_ok=True)
    for file_path in matched_files:
        file_name = str(file_path).rsplit('/', 1)[1]
        fix_pdb_file(file_path, f'{output_folder}/{file_name}')

    return

# 批量生成rfdiffusion创造的蛋白质骨架文件的二级结构文件（dssp）
def dssp_generation(input_folder,output_folder):
    filenames = os.listdir(input_folder)
    if not os.path.exists(output_folder):  ##新建文件夹
        os.makedirs(output_folder, exist_ok=True)
    for filename in filenames:
        file_name = filename.rsplit('.', 1)[0]
        run_dssp(f'{input_folder}/{filename}', f'{output_folder}/{file_name}.dssp')
        print(f'{file_name}.dssp generated successfully!')
    return

#从dssp文件中提取二级结构信息，存入csv文件
def dsspcsv(input_folder,output_folder):
    if not os.path.exists(output_folder):  ##新建文件夹
        os.makedirs(output_folder, exist_ok=True)
    filenames = os.listdir(input_folder)
    for filename in filenames:
        file_name = filename.rsplit('.', 1)[0]
        dssp_to_csv(f'{input_folder}/{filename}', f'{output_folder}/{file_name}.csv')
    return

#
def dssp_analyse(path, start_res, end_res, target_ss, threshold = 0.6):
    path_, useless, name_prefix= path.rsplit('/', 2)
    print('path_:', path_)
    print('useless_:', useless)
    print('name_prefix:', name_prefix)
    fix_pdb_folder = 'fix_pdb'
    dssp_folder = 'dssp_file'
    dssp_csv_folder = 'dssp_csv_file'
    print(os.path.exists(f'{path_}/{fix_pdb_folder}'))
    if not os.path.exists(f'{path_}/{fix_pdb_folder}'):
        os.makedirs(f'{path_}/{fix_pdb_folder}', exist_ok=True)
        print(f'{path_}/{fix_pdb_folder}')
    if not os.path.exists(f'{path_}/{dssp_folder}'):
        os.makedirs(f'{path_}/{dssp_folder}', exist_ok=True)
    if not os.path.exists(f'{path_}/{dssp_csv_folder}'):
        os.makedirs(f'{path_}/{dssp_csv_folder}', exist_ok=True)
    fix_pdb(path, f'{path_}/{fix_pdb_folder}')
    dssp_generation(f'{path_}/{fix_pdb_folder}', f'{path_}/{dssp_folder}')
    dsspcsv(f'{path_}/{dssp_folder}', f'{path_}/{dssp_csv_folder}')
    process_protein_files(f'{path_}/{dssp_csv_folder}', name_prefix,
                          f'{path_}/filter_results', start_res, end_res, target_ss, threshold, f'{path_}/fix_pdb')

    return


def check_ss_proportion(csv_path, start_res, end_res, target_ss_list, threshold):
    """
    检查蛋白质指定区域内多个二级结构的综合占比是否超过阈值

    参数:
    csv_path (str): CSV文件路径
    start_res (int): 起始残基序号
    end_res (int): 结束残基序号
    target_ss_list (list): 目标二级结构标识列表 (如['H', 'G'])
    threshold (float): 占比阈值(0-1之间)

    返回:
    tuple: (是否超过阈值, 实际占比, 区域内残基总数)
    """
    # 读取CSV文件
    df = pd.read_csv(csv_path)

    # 筛选指定残基范围内的数据
    region_df = df[(df['Residue_Number'] >= start_res) & (df['Residue_Number'] <= end_res)]

    # 计算区域内残基总数
    total_residues = len(region_df)

    # 如果区域内没有残基，返回False
    if total_residues == 0:
        return False, 0.0, 0

    # 计算目标二级结构的数量（多个标识）
    target_count = region_df[region_df['SS_8'].isin(target_ss_list)].shape[0]

    # 计算占比
    proportion = target_count / total_residues

    # 判断是否超过阈值
    exceeds_threshold = proportion > threshold

    return exceeds_threshold, proportion, total_residues

def process_protein_files(csv_folder, name_prefix,output_folder, start_res, end_res, target_ss, threshold, fixpdb_folder):
    """
    处理多个蛋白质文件，记录符合条件的蛋白质

    参数:
    file_list (list): 蛋白质CSV文件路径列表
    start_res (int): 起始残基序号
    end_res (int): 结束残基序号
    target_ss (str): 目标二级结构标识
    threshold (float): 占比阈值

    返回:
    list: 符合条件的蛋白质文件路径列表
    """
    positive_hits = []
    if not os.path.exists(output_folder):
        os.makedirs(output_folder, exist_ok=True)
    #file_list = os.listdir(csv_folder)
    #folder_path, file_prefix = path.rsplit('/', 1)
    pattern = re.compile(f"^{name_prefix}_\d+\.csv$")
    target_folder = Path(csv_folder)
    matched_files = [
        file for file in target_folder.iterdir()  # 遍历当前文件夹所有条目
        if file.is_file()  # 仅保留文件（过滤文件夹）
           and pattern.match(file.name)  # 正则匹配文件名
    ]
    matched_files = sorted(matched_files, key=natural_sort_key)
    
    
    with open(f'{output_folder}/record.txt', 'w') as f:
        for file_path in matched_files:
            exceeds, prop, count = check_ss_proportion(file_path, start_res, end_res, target_ss, threshold)
            if exceeds:
                print(f'Protein source path: {file_path}')
                print(f'Identification region: {start_res}-{end_res}')
                print(f'Proportion of secondary structure({target_ss}): {prop:.2%}')
                print(f'Exceeds threshold({threshold:.2%}): yes')
                print('')
                f.write(f'Protein source path: {file_path}\n')
                f.write(f'Identification region: {start_res}-{end_res}\n')
                f.write(f'Proportion of secondary structure({target_ss}): {prop:.2%}\n')
                f.write(f'Exceeds threshold({threshold:.2%}): yes\n')
                f.write(f'\n')

                positive_hits.append(file_path)
            else:
                print(f'Protein source path: {file_path}')
                print(f'Identification region: {start_res}-{end_res}')
                print(f'Proportion of secondary structure({target_ss}): {prop:.2%}')
                print(f'Exceeds threshold({threshold:.2%}): no')
                print('')
                f.write(f'Protein source path: {file_path}\n')
                f.write(f'Identification region: {start_res}-{end_res}\n')
                f.write(f'Proportion of secondary structure({target_ss}): {prop:.2%}\n')
                f.write(f'Exceeds threshold({threshold:.2%}): no\n')
                f.write(f'\n')

    for file_path in positive_hits:
        filename = str(file_path).rsplit('/',1)[1]
        file_name = filename.rsplit('.',1)[0]
        shutil.copy(f"{fixpdb_folder}/{file_name}.pdb", f'{output_folder}/{file_name}.pdb')
    return



def natural_sort_key(filename):
    """生成自然排序的key：将文件名拆分为字符串和数字部分，数字转整数"""
    parts = re.split(r'(\d+)', os.path.splitext(filename)[0])
    key = []
    for part in parts:
        if part.isdigit():
            key.append(int(part))
        else:
            key.append(part)
    return key

def report_csv(pdb_folder, dssp_csv_folder, start_res, end_res, ss, threshold, filter_folder, out_folder):
    """
    生成项目的摘要报告
    param pdb_folder:
    param dssp_csv_folder:
    param ss:
    param threshold:
    return:
    """

    import os
    import pandas as pd

    def csv_column_ratio_with_list(csv_path, start_idx, end_idx, column):
        """
        提取CSV指定列、指定行范围的元素列表，并统计各元素占比
        参数：
            csv_path: CSV文件路径（绝对/相对）
            start_idx: 开始索引（从1开始）
            end_idx: 结束索引（从1开始，包含）
            column: 列名（如"性别"）或列索引（从0开始，如0）
        返回：
            字典：
                - 元素列表：指定范围的原始元素（含空值，Python原生列表）
                - 占比统计：{元素: 占比（保留2位小数）}，强制包含H/E/C（无则0.00）
        示例：
            >>> result = csv_column_ratio_with_list("test.csv", 1, 10, "类型")
            >>> print(result["占比统计"])  # {'H': 0.30, 'E': 0.00, 'C': 0.50, 'A': 0.20}
        """
        # 异常处理：检查文件是否存在
        if not os.path.exists(csv_path):
            raise FileNotFoundError(f"CSV文件不存在：{csv_path}")

        # 1. 读取CSV文件（跳过空行，保留原始索引）
        try:
            df = pd.read_csv(csv_path, skip_blank_lines=True)
        except Exception as e:
            raise ValueError(f"读取CSV失败：{str(e)}")

        # 异常处理：空DataFrame
        if df.empty:
            raise ValueError("CSV文件无有效数据（空文件/全是空行）")

        # 2. 验证索引范围（从1开始→转换为0开始的切片）
        total_rows = len(df)
        if start_idx < 1 or end_idx < 1:
            raise ValueError("开始/结束索引必须≥1")
        if start_idx > end_idx:
            raise ValueError("开始索引不能大于结束索引")
        if end_idx > total_rows:
            raise ValueError(f"结束索引超出文件总行数（总行数：{total_rows}）")

        # 转换为0开始的切片（左闭右开，end_idx无需-1）
        slice_start = start_idx - 1
        slice_end = end_idx

        # 3. 提取指定列的指定行范围数据
        try:
            # 按列名/列索引提取列
            if isinstance(column, int):
                if column >= len(df.columns):
                    raise IndexError(f"列索引{column}超出范围（文件列数：{len(df.columns)}）")
                target_column = df.iloc[:, column]
            else:
                if column not in df.columns:
                    raise KeyError(f"列名'{column}'不存在（文件列名：{df.columns.tolist()}）")
                target_column = df[column]

            # ① 原始元素列表（含空值，保留原生类型）
            raw_element_list = target_column.iloc[slice_start:slice_end].tolist()
            # ② 去空值的列表（用于统计占比）
            clean_element_list = [x for x in raw_element_list if pd.notna(x)]

        except (KeyError, IndexError) as e:
            raise e
        except Exception as e:
            raise ValueError(f"提取列数据失败：{str(e)}")

        # 4. 统计元素占比（强制包含H/E/C，保留2位小数）
        required_elements = ['H', 'E', 'C']  # 必须包含的元素
        ratio_result = {}
        total_clean = len(clean_element_list)

        if total_clean == 0:
            # 无有效数据时，H/E/C占比均为0
            ratio_result = {elem: 0.00 for elem in required_elements}
        else:
            # 步骤1：初始化占比字典，先将H/E/C设为0
            ratio_result = {elem: 0.00 for elem in required_elements}
            # 步骤2：计算所有元素的占比（包括H/E/C和其他元素）
            count_series = pd.Series(clean_element_list).value_counts()
            for elem, count in count_series.items():
                ratio_result[str(elem)] = round(count / total_clean, 4)

        # 5. 按文档要求返回字典
        str_element = ''.join(raw_element_list)
        return str_element, ratio_result

    #index, design_ss8, design_ss3, h_prop, e_prop, c_prop,
    pdb_folder_path = Path(pdb_folder)  # 封装为Path对象
    csv_folder_path = Path(dssp_csv_folder)
    # 遍历文件夹，过滤出文件（仅当前目录）
    matched_pdb_files = [file.name for file in pdb_folder_path.iterdir() if file.is_file()]
    matched_pdb_files = sorted(matched_pdb_files, key=natural_sort_key)
    file_name_list = [os.path.splitext(filename)[0] for filename in matched_pdb_files]
    pdb_files_path_list = [os.path.join(pdb_folder_path, file) for file in matched_pdb_files]

    matched_csv_files = [file.name for file in csv_folder_path.iterdir() if file.is_file()]
    matched_csv_files = sorted(matched_csv_files, key=natural_sort_key)
    csv_files_path_list = [os.path.join(csv_folder_path, file) for file in matched_csv_files]
    index_list = []
    design_ss3 = []
    design_ss8 = []
    h_prop = []
    e_prop = []
    c_prop = []
    success_l = []
    backbone_l = []
    success_backbone_l = []
    if len(pdb_files_path_list) != len(csv_files_path_list):
        raise ValueError("pdb文件的数量与dssp文件对应的csv文件数量不符，请检查！")
    for pdb_file_path, csv_file_path, file_name, file in zip(pdb_files_path_list, csv_files_path_list, file_name_list, matched_pdb_files):
        index_list.append(file_name)
        design_seq8, useless = csv_column_ratio_with_list(
            csv_path=csv_file_path,
            start_idx=start_res,
            end_idx=end_res,
            column='SS_8'
        )
        design_seq, dict_prop = csv_column_ratio_with_list(
            csv_path=csv_file_path,
            start_idx=start_res,
            end_idx=end_res,
            column='SS_3'
        )
        design_ss8.append(design_seq8)
        design_ss3.append(design_seq)
        for key, value in dict_prop.items():
            if key == 'H':
                h_prop.append(value)
            if key == 'E':
                e_prop.append(value)
            if key == 'C':
                c_prop.append(value)

        if ss == 'helix':
            if h_prop[-1] < threshold:
                success_l.append('No')
                success_backbone_l.append('-')
            else:
                success_l.append('Yes')
                success_backbone_l.append(os.path.join(filter_folder, file))
        if ss == 'strand':
            if e_prop[-1] < threshold:
                success_l.append('No')
                success_backbone_l.append('-')
            else:
                success_l.append('Yes')
                success_backbone_l.append(os.path.join(filter_folder, file))
        if ss == 'coil':
            if c_prop[-1] < threshold:
                success_l.append('No')
                success_backbone_l.append('-')
            else:
                success_l.append('Yes')
                success_backbone_l.append(os.path.join(filter_folder, file))
        backbone_l.append(pdb_file_path)
    segment = [f'{start_res}-{end_res}']*len(index_list)
    df_data = {
        "index": index_list,
        'segment': segment,
        "design_ss8": design_ss8,
        "design_ss3": design_ss3,
        "H_prop": h_prop,
        "E_prop": e_prop,
        "C_prop": c_prop,
        "Success": success_l,
        "backbone": backbone_l,
        "success_backbone": success_backbone_l
    }
    df_data = pd.DataFrame(df_data)
    df_data.to_csv(f'{out_folder}/rfdiffusion_report.csv', index=False)
    return
















if __name__ == '__main__':
    args = parse_args()
    rfdiffusion_prefix = os.path.expanduser(args.rfdiffusion_prefix)
    inpaint_str = args.inpaint_str
    ss = args.ss
    threshold = args.threshold
    final_report_folder = os.path.expanduser(args.final_report_folder)

    pattern = r'^\[?\'?[a-zA-Z]+(\d+)-(\d+)\'?\]?$'
    match = re.fullmatch(pattern, inpaint_str.strip())
    if match:
        # 捕获组1是第一个数字，捕获组2是第二个数字，转为整数返回
        start_res = int(match.group(1))
        end_res = int(match.group(2))
    else:
        print(f"Parameter error in contigmap.inpaint_seq: {inpaint_str}")

    # target_ss = ['H']
    if ss == 'helix':
        target_ss = ['H', 'G', 'I', 'P']
    if ss == 'strand':
        target_ss = ['E', 'B']
    if ss == 'coil':
        target_ss = ['S', 'T', 'C']

    dssp_analyse(rfdiffusion_prefix, start_res, end_res, target_ss, threshold=threshold)

    work_dir, rfdiffusion_out, useless1, useless2 = rfdiffusion_prefix.rsplit('/', 3)
    pdb_folder = os.path.join(work_dir, rfdiffusion_out, 'fix_pdb')
    dssp_csv_folder = os.path.join(work_dir, rfdiffusion_out, 'dssp_csv_file')
    filter_folder = os.path.join(work_dir, rfdiffusion_out, 'filter_results')
    if final_report_folder is None:
        out_folder = final_report_folder
    else:
        out_folder = work_dir
    report_csv(
        pdb_folder=pdb_folder,
        dssp_csv_folder = dssp_csv_folder,
        start_res = start_res,
        end_res = end_res,
        ss = ss,
        threshold = threshold,
        filter_folder = filter_folder,
        out_folder = out_folder,
    )
    
    

